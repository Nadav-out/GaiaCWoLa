{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/anaconda3/envs/shared_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/envs/shared_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/envs/shared_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/envs/shared_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/envs/shared_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/envs/shared_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "from sklearn.metrics import roc_curve, auc,roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from matplotlib import gridspec\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "import keras\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(allow_growth=True, per_process_gpu_memory_fraction=0.5)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_arrays(SR, SB, gaiadata2):\n",
    "    X = SR[:,3]-center_ra\n",
    "    Y = SR[:,2]-center_dec\n",
    "\n",
    "    Xb = SB[:,3]-center_ra\n",
    "    Yb = SB[:,2]-center_dec\n",
    "    \n",
    "    Xad = gaiadata2[:, 3]-center_ra\n",
    "    Yad = gaiadata2[:, 2]-center_dec\n",
    "\n",
    "    SR = np.c_[SR[:,0],SR[:,1],X, Y, SR[:,4],SR[:,5]]\n",
    "    SB = np.c_[SB[:,0],SB[:,1],Xb, Yb, SB[:,4],SB[:,5]]\n",
    "    gaiadata2 = np.c_[gaiadata2[:,0],gaiadata2[:,1], Xad, Yad, gaiadata2[:,4],gaiadata2[:,5]]\n",
    "    \n",
    "    return SR, SB, gaiadata2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_arr_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_diff(SR, SB):\n",
    "    return abs(len(SR) - len(SB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_SR_SB_pointers(pointers, increment, data_arr, start_point, end_point):\n",
    "    SR = data_arr[(data_arr[:,0] > pointers[1])*(data_arr[:,0] < pointers[2])]\n",
    "    SB = data_arr[(data_arr[:,0] > pointers[0])*(data_arr[:,0] < pointers[1]) + (data_arr[:,0] > pointers[2])*(data_arr[:,0] < pointers[3])]\n",
    "\n",
    "    previous_diff = calc_diff(SR, SB)\n",
    "    prev_SR, prev_SB = SR, SB\n",
    "    curr_diff = previous_diff\n",
    "    bottom_bool = False\n",
    "    top_bool = True\n",
    "\n",
    "    while curr_diff <= previous_diff and pointers[3] < end_point:\n",
    "        previous_diff, prev_SR, prev_SB = curr_diff, SR, SB\n",
    "        if bottom_bool:\n",
    "            pointers[0] = max(start_point, pointers[0] - increment)\n",
    "        else:\n",
    "            pointers[3] = min(end_point, pointers[3] + increment)\n",
    "        bottom_bool, top_bool = top_bool, bottom_bool\n",
    "        SR = data_arr[(data_arr[:,0] > pointers[1])*(data_arr[:,0] < pointers[2])]\n",
    "        SB = data_arr[(data_arr[:,0] > pointers[0])*(data_arr[:,0] < pointers[1]) + (data_arr[:,0] > pointers[2])*(data_arr[:,0] < pointers[3])]\n",
    "        curr_diff = calc_diff(SR, SB)\n",
    "\n",
    "    return prev_SR, prev_SB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angular_distance(angle1,angle2):\n",
    "    # inputs are np arrays of [ra,dec]\n",
    "    deltara=np.minimum(np.minimum(np.abs(angle1[:,0]-angle2[:,0]+360),np.abs(angle1[:,0]-angle2[:,0])),\\\n",
    "                          np.abs(angle1[:,0]-angle2[:,0]-360))\n",
    "    deltadec=np.abs(angle1[:,1]-angle2[:,1])\n",
    "    return np.sqrt(deltara**2+deltadec**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function from David's file via_machinae.py\n",
    "def FilterGD1(stars):\n",
    "    gd1stars=np.zeros(len(stars))\n",
    "    for x in allgd1stars:\n",
    "        ra=x[0]\n",
    "        dec=x[1]\n",
    "        pmra=x[2]\n",
    "        pmdec=x[3]\n",
    "    \n",
    "        foundlist=angular_distance(np.dstack((stars[:,3],stars[:,2]))[0],np.array([[ra,dec]]))\n",
    "        foundlist=np.sqrt(foundlist**2+(stars[:,0]-pmdec)**2+(stars[:,1]-pmra)**2)   \n",
    "        foundlist=foundlist<.0001\n",
    "        if len(np.argwhere(foundlist))>1:\n",
    "            print(foundlist)\n",
    "        if len(np.argwhere(foundlist))==1:\n",
    "            gd1stars+=foundlist\n",
    "    gd1stars=gd1stars.astype('bool')\n",
    "    return gd1stars,stars[gd1stars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = '/data0/users/bpnachman/Gaia/gaiascan_l101.2_b58.4_ra212.7_dec55.2.npy'\n",
    "gaiadata=np.load(datafile,allow_pickle=True)\n",
    "gaiadata2 = np.array(gaiadata[:,[9,8,6,7,4,5]]).astype('float32') \n",
    "#pm_lat, pm_lon_coslat, lon, lat, color, mag\n",
    "allgd1stars = np.load('gd1_stars.npy')\n",
    "is_stream_arr, gaiadata3 = FilterGD1(gaiadata)\n",
    "\n",
    "new_gaiadata = []\n",
    "for i in range(len(gaiadata)):\n",
    "    temp = np.append(gaiadata[i], is_stream_arr[i])\n",
    "    new_gaiadata.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaiadata = new_gaiadata\n",
    "gaiadata = np.array(gaiadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#switch 0 and 1\n",
    "gaiadata2 = np.array(gaiadata[:,[8,9,6,7,4,5,10]]).astype('float32') \n",
    "gaiadata3 = np.array(gaiadata3[:,[8,9,6,7,4,5]]).astype('float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaiadata2 = gaiadata2[(np.abs(gaiadata2[:,0]) > 2) + (np.abs(gaiadata2[:,1]) > 2)] \n",
    "gaiadata2 = gaiadata2[(gaiadata2[:,4]>0.5) * (gaiadata2[:,4]<1)]\n",
    "\n",
    "gaiadata3 = gaiadata3[(np.abs(gaiadata3[:,0]) > 2) + (np.abs(gaiadata3[:,1]) > 2)] \n",
    "gaiadata3 = gaiadata3[(gaiadata3[:,4]>0.5) * (gaiadata3[:,4]<1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fractional_background(all_data, fraction):\n",
    "    num = fraction*100\n",
    "    print(num)\n",
    "    frac_background = []\n",
    "    for i in range(len(all_data)):\n",
    "        if all_data[i][6]:\n",
    "            frac_background.append(all_data[i])\n",
    "        else:\n",
    "            if random.randint(0, 100) < num:\n",
    "                frac_background.append(all_data[i])\n",
    "    frac_background = np.array(frac_background)\n",
    "    frac_background = frac_background[:, :6]\n",
    "    return frac_background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_background = make_fractional_background(gaiadata2, 0.5)\n",
    "\n",
    "\n",
    "SR = frac_background[(frac_background[:,0] > -10)*(frac_background[:,0] < -8)]\n",
    "SB = frac_background[(frac_background[:,0] > -11)*(frac_background[:,0] < -10)+(frac_background[:,0] > -8)*(frac_background[:,0] < -7)]\n",
    "print(len(SR), len(SB))\n",
    "pointer1 = -10\n",
    "pointer2 = -8\n",
    "all_data = frac_background\n",
    "stream = gaiadata3_2\n",
    "\n",
    "X = np.concatenate([SR,SB])\n",
    "Y = np.concatenate([np.ones(len(SR)),np.zeros(len(SB))])\n",
    "\n",
    "myscalar = preprocessing.StandardScaler()\n",
    "myscalar.fit(X)\n",
    "X_scaled = myscalar.transform(X)\n",
    "all_data_scaled = myscalar.transform(all_data)\n",
    "\n",
    "X_scaled = X_scaled[Y<2]\n",
    "Y = Y[Y<2]\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.5)\n",
    "\n",
    "print('working')\n",
    "\n",
    "#set biases for each layer, bias_initializer\n",
    "#initializers.GlorotNormal()\n",
    "\n",
    "preds = []\n",
    "preds_all = []\n",
    "num_loops = 5\n",
    "\n",
    "for _ in range(num_loops):\n",
    "    model = Sequential()\n",
    "    initializer =tf.keras.initializers.glorot_normal()\n",
    "    model.add(Dense(256, input_dim=5, activation='relu', bias_initializer = initializer)) \n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(256, activation='relu', bias_initializer = initializer))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(256, activation='relu', bias_initializer = initializer))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(1, activation='sigmoid', bias_initializer = initializer))\n",
    "    optimizer = keras.optimizers.Adam(lr=1e-4)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    history = model.fit(X_train[:,1:],Y_train, epochs=200, batch_size=200,validation_data=(X_test[:,1:],Y_test), verbose = 0) \n",
    "\n",
    "    preds_curr = model.predict(X_test[:,1:], batch_size=int(0.1*len(X_test)))\n",
    "    preds_all_curr = model.predict(all_data_scaled[:, 1:])\n",
    "    \n",
    "    if len(preds) == 0:\n",
    "        preds = preds_curr\n",
    "        preds_all = preds_all_curr\n",
    "    else:\n",
    "        preds = preds + preds_curr\n",
    "        preds_all = preds_all + preds_all_curr\n",
    "\n",
    "preds = [x/num_loops for x in preds]\n",
    "preds_all = [x/num_loops for x in preds_all]\n",
    "\n",
    "preds = np.array(preds)\n",
    "preds_all = np.array(preds_all)\n",
    "    \n",
    "stars_passing_cut = []\n",
    "\n",
    "X_test_unscaled = myscalar.inverse_transform(X_test)\n",
    "\n",
    "preds_sorted = preds[np.argsort(preds[:,0])]\n",
    "X_test_sorted = X_test_unscaled[np.argsort(preds[:,0])]\n",
    "X_test_sorted = X_test_sorted[(X_test_sorted[:,0] > pointer1) * (X_test_sorted[:,0] < pointer2)]\n",
    "\n",
    "stars_passing_cut = X_test_sorted[len(X_test_sorted) - 100:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(stars_passing_cut[:,3],stars_passing_cut[:,2], marker = '.')\n",
    "plt.scatter(gaiadata3[:,3], gaiadata3[:,2], marker = '.', alpha = 0.2)\n",
    "plt.xlim(-15,15)\n",
    "plt.ylim(-15,15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], color = 'blue', label = 'loss')\n",
    "plt.plot(history.history['val_loss'], color = 'red', label = 'val loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
